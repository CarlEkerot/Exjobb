\documentclass[a4paper]{report}

\usepackage[round]{natbib}
\usepackage[nottoc,numbib]{tocbibind}
\usepackage{graphicx}
\usepackage[noline,noend,algoruled]{algorithm2e}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[small]{caption}
\usepackage{subcaption}
\usepackage[table]{xcolor}
\usepackage[
    pdftitle={Automatic Structural Inference of Binary Protocols},
    pdfauthor={Fredrik Appelros, Carl Ekerot}
]{hyperref}
\usepackage{bytefield}
\usepackage{afterpage}

% Title page
\RequirePackage{titling}
\makeatletter
\newcommand{\company}[1]{\def \@company {#1}}
\newcommand{\supervisor}[1]{\def \@supervisor {#1}}
\newcommand{\examiner}[1]{\def \@examiner {#1}}
\newcommand{\subtitle}[1]{\def \@subtitle {#1}}
\company{}
\supervisor{}
\examiner{}
\subtitle{}
\pretitle{\noindent\rule{\linewidth}{1pt}\begin{center}\Huge}
\posttitle{\par \vskip 0.5em \ifx \@subtitle \empty \vskip 1em \else {\Large(\@subtitle)} \fi \end{center}\noindent\rule{\linewidth}{1pt}\vskip 0.5em}
\predate{\vskip 5em \begin{center}\Large}
\postdate{\par\vfill {\large Master's thesis work carried out at 
\ifx \@company \empty
\ClassWarning{cslthse-msc}{Host company missing. Use \protect\company{name}. Defaulting to CS.}
\\ the Department of Computer Science, Lund University.
\else \@company. \fi } \\  
\vskip 1em {\normalsize Supervisor: 
\ifx \@supervisor \empty
\ClassError{cslthse-msc}{Missing supervisor. Use \protect\supervisor{name,email}}{You must specify your thesis supervisor.} 
\else \@supervisor \fi \\ Examiner: 
\ifx \@examiner \empty
\ClassError{cslthse-msc}{Missing examiner. Use \protect\examiner{name,email}}{You must specify your thesis examiner.}
\else    \@examiner \fi } \end{center}}
\makeatother

% Command to create blank page.
\newcommand\blankpage{%
    \null
    \thispagestyle{empty}%
    \addtocounter{page}{-1}%
    \newpage}

% fancy headers, footers
\RequirePackage{fancyhdr}
\makeatletter
\pagestyle{fancy}
\renewcommand{\chaptermark}[1]{\markboth{\thechapter.\ #1}{}}
\renewcommand{\sectionmark}[1]{\markright{\thesection\ #1}{}}
\fancyhf{}
\fancyhead[LE]{\footnotesize{\textsc{\leftmark}}}
\fancyhead[RO]{\footnotesize{\textsc{\rightmark}}}
\fancyfoot[LE,RO]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

\fancypagestyle{plain}{
  \fancyhead{}
  \renewcommand{\headrulewidth}{0pt}
%  \renewcommand{\footrulewidth}{0pt}
}
\makeatother


\newtheorem*{definition}{Definition}
\defcitealias{intel12}{Intel\textsuperscript{\textregistered} 64 and IA-32
Architectures Optimization Reference Manual}
\defcitealias{droms97}{RFC 2131}
\defcitealias{sollins92}{RFC 1350}
\defcitealias{mockapetris87}{RFC 1035}
\defcitealias{schulzrinne03}{RFC 3550}
\defcitealias{microsoft13}{[MS-SMB]: Server Message Block (SMB) Protocol}

\begin{document}

\title{Automatic Structural Inference of Binary Protocols}
\author{Fredrik Appelros \and Carl Ekerot}
\company{Procera Networks}
\supervisor{Anders Waldenborg}
\examiner{Pierre Nugues}
\date{\today}
\maketitle
\blankpage

\begin{abstract}
Communication protocols are the foundation of everyday networking tasks but
they are not always open in terms of available documentation. For reasons of
interoperability or the collection of statistics some protocols need to be
reverse engineered. This thesis presents a method based on density-based
clustering and byte distribution classification for automatically inferring the
structure of a protocol from a packet dump. We evaluate the precision of the
method for finding different packet types and for finding fields globally
across all packets.
\end{abstract}
\blankpage

\tableofcontents

\chapter{Introduction}

\section{Background}
Communication protocols are the foundation of everyday networking tasks. Each
time a web page is served or an email is sent, a rigorous series of actions is
taken to ensure that the requested piece of information is relayed. The
information is packaged in a \emph{message} and both the syntax and the
semantics of these messages are defined by protocols.

Protocols may be open or closed in terms of available documentation. The reason
for them being closed might be that they are proprietary or simply because no
one invested in creating a documentation of the protocol in the first place.
Despite the lack of a documentation, there still sometimes exists a need to
understand the inner workings of a certain protocol, e.g. when providing
interoperability between services. One infamous example of this is Microsoft's
proprietary SMB protocol which spawned the Samba project. This project was
created in an attempt to provide access to certain Windows services for
Unix-like operating systems. \citet{tridgell03} explains that the Samba project
reversed the SMB specification using techniques like \emph{fuzzing}, a process
where they inspect the response from a SMB server for a wide variety of
generated requests. Using this approach, it took many years until most of the
functionality of the SMB protocol had been implemented.

Another area where the need for inferring the structures of closed protocols is
in \emph{deep packet inspection} (DPI). Messages sent over a network are
usually constructed hierarchically. At the bottom there is only raw binary data
and at the top there is the application data, e.g. an email or a video. There
in between exists several layers that contain metadata which is used to make
sure that the data reaches its target and that it does not get corrupted during
transmission. As a message is sent from one party to another, it traverses a
series of nodes in the network. These nodes only need to examine the lower
parts of the hierarchical structure of the message in order to relay it. If a
node wants to conduct DPI however, the higher levels of the hierarchy are
examined as well. To be able to do this the structure of the protocol needs to
be known.

There are many different reasons why one would want to perform DPI, one of
which is to provide \emph{quality of service} (QoS) solutions. QoS solutions
are needed in order to gather statistics for Internet service providers that
manage the networks.

In this thesis, we have focused on finding the structure of binary protocols as
opposed to textual ones. The reason for this being that textual protocols are
intrinsically simple to read and understand and therefore do not require
advanced methods of analysis. Some of the methods described here are however
also applicable to textual protocols.

\section{Related work}
A popular approach to the problem of automatic inference of network protocols
has been to draw a parallel to bioinformatics. Byte-streams have been
processed using the same methodology as DNA sequence analysis in order
to mine patterns in protocol messages. \citet{beddoe05} explores the
possibilities of using bioinformatics methods such as multiple sequence
alignment in order to find patterns in both textual and binary protocols. These
principles have been applied in e.g. Netzob\footnote{http://www.netzob.org}, a
tool for protocol analysis.

Using techniques from bioinformatics in protocol analysis seems natural since
the analysis of messages, which may be viewed as the analysis of byte-streams,
is easily converted to the problem of sequence analysis. Methods such as
multiple sequence alignment do provide an insight to the structure of
protocols, but further analysis is needed in order to find semantic meaning in
segments of bytes.

Discoverer by \citet{cui07} uses an approach which is specifically modeled
after the behavior of network protocols. Discoverer tokenizes
network dumps into textual and binary tokens. From the tokens Discoverer
attempts to group messages based on their true message type, such as requests
or responses. After the grouping of messages, Discoverer seeks semantic meaning
in the tokens in common for each group of messages, and classifies the tokens
into a set of predefined classes.

\citet{caballero12} propose a method where protocol specifications are
constructed by monitoring the execution state of the client or server software
which utilizes the protocol. This approach has the obvious benefit of mapping
data stored in memory at one state to the data subsequently present in a
protocol message. This requires access to the actual software which generates
the messages. In many cases, this may limit the analysis to the messages sent by
the client. When the server software is unavailable for execution state
monitoring, one must resort to other methods in order to further analyze the
protocol.

For our purposes we need a method that can be applied on observed data alone,
without access to the actual software that generates the traffic. This leads us
to a method similar to the one used in Discoverer. However, the approach used
in Discoverer is severely limited as it depends upon finding two types of
tokens; binary and text tokens. Since the method is based on the results of
this tokenization, it is not suitable for protocols that are mostly or entirely
binary. Our approach is designed to work well with these types of protocols,
and additionally to find a more detailed semantic meaning for each field.

\chapter{Theory}
In this chapter, we present the theory behind the techniques used throughout
this report. The theory is divided into four subjects: features, clustering,
sequence alignment and simple linear regression. The first two subjects relate
to the techniques used in Section~\ref{sec:type_inf} while the latter two
relate to the techniques used in Section~\ref{sec:field_inf}.

\section{Features}
In machine learning, a feature is a measurable property of the input data, e.g.
the intensity of light for a certain pixel when working with image data.
Features do not need to directly correspond to a physically measurable property
however, instead they can be constructed by transforming other features. These
new high-level features can then be used just like any other feature in order
to simplify a complex problem.

One common type of complex problem is working with high dimensional data. There
are different ways to deal with such data; one of them being to use a solving
method that scales linearly. Another way is to reduce the dimensionality of the
problem itself.

\subsection{Principal component analysis}
\label{sec:pca}
One method to reduce the dimensionality of a problem is to use \emph{principal
component analysis} (PCA). PCA was first defined by \citet{pearson01} and
transforms the input data into another dataset where each dimension accounts
for as much variance as possible while requiring it to maintain orthogonality
to the other dimensions. The new dataset can be constructed to contain fewer
dimensions than the original data and will then capture as much variance as
possible in the given number of dimensions. PCA can be accomplished through
singular value decomposition of a matrix.

\section{Clustering}
A clustering is a grouping of samples based on similarity with respect to their
features. The features are defined to represent distinct properties of the
samples. Similarities between samples are normally represented as distances in
an $n$-dimensional space, where $n$ is the number of features. Distances may be
calculated using any suitable norm.

In cluster analysis, the goal is normally to find a clustering from a
set of samples such that the membership of a cluster represents some true
relationship. In some clustering algorithms such as K-means \citep{macqueen67},
the number of clusters are predefined. When the number of clusters are unknown,
other methods such as density-based or hierarchical clustering algorithms may
be used.

The clustering algorithm we use in our method is OPTICS, which is based on
DBSCAN. In the rest of this section we will explain these algorithms and
provide examples based on protocol data.

\subsection{DBSCAN}
DBSCAN is a \emph{density-based} clustering algorithm, as opposed to
partitioning clustering algorithms such as K-means. Density-based clustering
algorithms provide the benefit of not having to provide an estimated number
of clusters as a parameter to the algorithm.

DBSCAN was first presented by \citet{ester96}. The algorithm takes a set of
samples $D$ and two parameters, $MinPts$ and $\varepsilon$. $MinPts$ represents
the lower bound for how many proximal samples are needed in order to form a
cluster, while $\varepsilon$ is a parameter used to define proximity. In order
to explain the algorithm in greater detail we first need to introduce some
definitions.

\begin{definition}
The \emph{$\varepsilon$-neighborhood}, $N_{\varepsilon}(p)$, for a sample $p$
is defined as the following:
\[
    N_{\varepsilon}(p) = \{ q \in D ~|~ dist(p,q) \le \varepsilon  \}
    \label{eq:eps}
\]
where $dist(p,q)$ is the distance between samples $p$ and $q$.
\end{definition}

Furthermore, the samples in $N_{\varepsilon}(p)$ are defined as \emph{directly
density-reachable} from $p$. Given that $|N_{\varepsilon}(p)| \ge MinPts$, the
samples in $N_{\varepsilon}(p)$ forms a cluster, and $p$ is a \emph{core
sample}. A cluster is not limited to containing samples which are directly
density-reachable. The DBSCAN algorithm also defines that samples which are not
directly density-reachable may be \emph{density-reachable}. A sample $q$ is
density-reachable from a sample $p$ if there is a set of samples
$S = \{s_1, s_2, \ldots, s_n\}$ where $p = s_1$, $q = s_n$ and $s_{i+1}$ is
directly density-reachable from $s_i$ for $0 < i < n$.

Since the density-reachable relationship is not symmetric, a looser
relationship is introduced: \emph{density-connected}. Two samples $p$ and
$q$ are density-connected if there exists a third sample $o$ from which both
$p$ and $q$ are density-reachable. The density-connected relationship
is symmetric.

With these relationships, the algorithm defines cluster membership as
follows:

\begin{definition}
    The following conditions needs to be satisfied for a sample $q$ to be a
    member of a cluster $C$, given that there is a sample $p \in C$:
    \begin{enumerate}
        \item $q$ is density-reachable from $p$
        \item $q$ is density-connected to $p$
    \end{enumerate}
\end{definition}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{img/dbscan_dns}
    \captionsetup{width=0.8\textwidth}
    \caption{An example of a clustering constructed by running DBSCAN on
    samples generated from 5000 packets of DNS data. The samples have been
    projected down to two dimensions using PCA (see Section~\ref{sec:pca}).}
    \label{fig:dbscan}
\end{figure}

One of the drawbacks of DBSCAN is that it can only find clusters with a density
higher than the density given by the $MinPts$ and $\varepsilon$ parameters.
It is also hard to estimate the parameters for an unknown dataset. This makes
DBSCAN suitable for classifying data into known classes, but not for finding
underlying structures in unknown datasets. In Figure~\ref{fig:dbscan} there is
an example of running DBSCAN on 5000 packets of DNS data. The parameters were
manually selected as $MinPts = 200$ and $\varepsilon = 0.085$.

\subsection{OPTICS}
Addressing the difficulties in selecting parameters for DBSCAN,
\citet{ankerst99} introduced the OPTICS algorithm, which is an extension of
the concepts introduced in DBSCAN. OPTICS uses the definitions
\emph{directly density-reachable}, \emph{density-reachable} and
\emph{density-connected} which were described in the DBSCAN algorithm, but
eliminates the need for an explicit $\varepsilon$ parameter. Instead the
$\varepsilon$ parameter is interpreted as the largest distance to consider
when clustering. The result is an algorithm which is less sensitive to
user-specified parameters, and is able to find clusters with varying densities.

One important aspect of OPTICS is that it does not generate actual clusters.
Instead, it generates an \emph{ordering} of samples and a set of corresponding
\emph{reachability-distances} which reveals density-based structure in the
input data. The OPTICS algorithm extends DBSCAN with the definitions
\emph{core-distance} and \emph{reachability-distance}.

Core-distance is a measure of the distance $\varepsilon$ which is required
for a sample $p$ to be a core sample. The $\varepsilon$ parameter can be set to
$\infty$ but that might affect the runtime of the algorithm as more samples are
considered for inclusion in the clusters. Reachability-distance is the smallest
distance such that $p$ is directly density-reachable from a core sample $q$.

\begin{definition}
    The core-distance for a sample $p$ is defined as:
\[
    \begin{cases}
        \text{UNDEFINED}, & \text{if $|N_{\varepsilon}(p)| < MinPts$}\\
        \text{Min $\varepsilon$ which satisfies $|N_{\varepsilon}(p)| \ge
        MinPts$}(p), & \text{otherwise}
    \end{cases}
\]
\end{definition}

\begin{definition}
    The reachability-distance of a sample $p$ with respect to some sample $q$
    is defined as:
\[
    \begin{cases}
        \text{UNDEFINED}, & \text{if $|N_{\varepsilon}(q)| < MinPts$}\\
        \text{max(core-distance($q$), distance($q$, $p$))}, & \text{otherwise}
    \end{cases}
\]
    where distance($q$, $p$) is the smallest distance for which $q$ is
    density-reachable from $p$.
\end{definition}

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{img/rplot_dhcp}
        \caption{Reachability-plot for 5000 DHCP-packets with $MinPts = 200$.}
        \label{fig:rplot}
    \end{subfigure}
    \quad
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{img/hierextr}
        \caption{Hierarchical extraction for 5000 DNS-packets with
        $MinPts = 200$.}
        \label{fig:hierextr}
    \end{subfigure}
    \caption{Reachability-plots for DHCP and DNS data. In (\subref{fig:rplot})
        a normal reachability plot for DHCP data is given. In
        (\subref{fig:hierextr}) a similar, but more complex, plot is given for
        DNS data. The alternating background color represents different clusters
        that have been extracted. The true types \emph{request} and
        \emph{response} are colored black and gray respectively. Note that the
        true types of DNS may be extended to different record types such as
        A/AAAA/CNAME-records.}
    \label{fig:rplots}
\end{figure}

From the ordering and reachability-distances, the cluster structure may be
visualized in a \emph{reachability-plot}. The reachability-plot provides an
overview of the output from OPTICS as a bar-plot where the bars represent
reachability-distances in the ordering generated by OPTICS. Valleys in the
reachability-plot represent samples which are close to each other. Spikes
represent a large difference in distance between the samples to the left and
right of the spike.

An example of a reachability plot generated from 5000 DHCP-packets with
$MinPts = 500$ is shown in Figure~\ref{fig:rplot}. The pseudocode for OPTICS
is given in Algorithm~\ref{alg:optics}.
\\
{
    \fontsize{10}{12}
    \selectfont
    \begin{algorithm}[H]
        \DontPrintSemicolon
        \BlankLine
        \KwIn{$D$, $MinPts$, $\varepsilon$}
        \KwOut{$ordering$, $reachability$-$distances$}
        \BlankLine

        $reachability$-$distances \gets \varnothing$\;
        $ordering \gets \varnothing$\;
        $seeds \gets 0,1,...,|D|$\;
        $i \gets 0$\;

        \While{$|seeds| > 1$} {
            $p \gets seeds.get(i)$\;
            $seeds.remove(i)$\;
            $ordering.add(p)$\;

            \If{$core$-$distance_{\varepsilon, MinPts}(p) \le \varepsilon$}{
                $neighbors \gets N_{\varepsilon}(p)$\;
                \For{each neighbor sample $n$} {
                    $rdist \gets \text{$reachabilty$-
                        $distance_{\varepsilon, MinPts}$}(n,p)$\;
                    $reachability$-$distances[n] \gets rdist$\;
                }
                $i \gets$ seed with least reachability-distance\;
            }
            \Else{
                $i \gets seeds.first$\;
            }
        }
        \CommentSty{/* Process last remaining seed */}\;
        $ordering.add(seeds.first)$\;
        $reachability$-$distances[0] \gets 0$\;
        \Return{$ordering$, $reachability$-$distances$}
        \BlankLine
        \caption{OPTICS}
        \label{alg:optics}
    \end{algorithm}
}

\subsubsection{Cluster extraction}

\citeauthor{ankerst99} describe the principles of the OPTICS algorithm as
running DBSCAN for an infinite number of distance parameters $\varepsilon_i$
in the interval $0 \le \varepsilon_i \le \varepsilon$. They also explain that
extracting clusters from the OPTICS ordering and reachability-distances using
a static reachability-distance threshold $\varepsilon_i$ gives a clustering
which is roughly equivalent to the clustering obtained when running DBSCAN
with the same $MinPts$ and $\varepsilon = \varepsilon_i$.

\citet{sander03} describe a hierarchical cluster extraction algorithm for
OPTICS. As opposed to the DBSCAN-equivalent extraction approach, hierarchical
cluster extraction retains many of the properties which comes with OPTICS,
such as finding clusters with varying densities and subclusters nested in larger
clusters.

The algorithm takes an ordering and reachability-distances from OPTICS and
builds a hierarchical representation of the clustering in the form of a tree,
where the leaves are clusters.

The pseudocode for hierarchical extraction is given in
Algorithm~\ref{alg:hierextr}. Figure~\ref{fig:hierextr} is the result of
the algorithm running on the same DNS dataset as Figure~\ref{fig:dbscan}
with $MinPts = 200$. Note that the algorithm is rather insensitive to the
$MinPts$ parameter.
\\
{
    \fontsize{10}{12}
    \selectfont
    \begin{algorithm}[H]
        \DontPrintSemicolon
        \BlankLine
        \SetKwBlock{Fn}{function}{end}
        \KwIn{$ordering$, $reachability$-$distances$}
        \KwOut{$clustering$}
        \BlankLine

        $R \gets reachability$-$distances$ arranged according to $ordering$\;
        $n \gets$ number of samples\;
        $L \gets$ indices of local maxima in $R$\;
        Sort $L$ on $R[L_i]$\;
        $R_{max} \gets$ $R[L.last]$\;
        $leaves \gets \varnothing$\;

        \Fn(\emph{cluster\_tree(node, parent, L)}:){
            \If{$L$ is empty}{
                $leaves.add(node)$\;
                \Return\;
            }
            $s \gets L.pop()$\;
            $node.split \gets s$\;
            $sign\_thres \gets significant$-$ratio * R[s]$\;
            Create two new nodes, $N1$ and $N2$\;
            $N1.samples \gets$ samples left of $s$\;
            $N2.samples \gets$ samples right of $s$\;
            $L1 \gets $ local maxima left of $s$\;
            $L2 \gets $ local maxima right of $s$\;
            \If{$N1$ and $N2$ has average reachability $<$ $sign\_thres$}{
                \If{$|N1| > MinPts$}{
                    $children.add(\{N1, L1\})$\;
                }
                \If{$|N2| > MinPts$}{
                    $children.add(\{N2, L2\})$\;
                }
                \If{$children$ is empty}{
                    $leaves.add(node)$\;
                    \Return\;
                }
                \If{$R[s] \approx R[parent.split]$}{
                    \For{$\{child, L\}$ in $children$}{
                        $parent.children.add(child)$\;
                    }
                    $parent.children.remove(node)$\;
                    $p \gets parent$\;
                }
                \Else{
                    \For{$\{child, L\}$ in $children$}{
                        $node.children.add(child)$\;
                    }
                    $p \gets node$\;
                }
                \For{$\{child, L\}$ in $children$}{
                    \emph{cluster\_tree(child, p, L)}\;
                }
            }
            \Else{
                \emph{cluster\_tree(node, parent, L)}\;
            }
        }
        \BlankLine
        Create node $root$ containing all samples\;
        \emph{cluster\_tree(root, null, L)}\;
        \BlankLine
        \caption{Hierarchical Cluster Extraction}
        \label{alg:hierextr}
    \end{algorithm}
}

\subsection{Clustering metrics}
\label{sec:metrics}
For measuring the quality of a clustering, a number of metrics need to be
defined. These metrics reflect the quality of the features as well as the
clustering algorithm. The metrics require labels given from the clustering
algorithm as well as a \emph{truth}, i.e. the true classes of the samples.
The clustering metrics we use are based on information entropy theory and
for their exact definition we refer to the paper by \citet{rosenberg07}
where they were originally defined. Below are intuitive descriptions of
each metric.

\subsubsection{Homogenity score}
Homogenity, $h$, is satisfied when every cluster only contains members of a
single true class. The homogeneity score lies in the range $0 \le h \le 1$ and
when completely satisfied, $h = 1$.

\subsubsection{Completeness score}
Completeness, $c$, is satisfied when all members of a true class are in the
same cluster. Like the homogeneity score, completeness lies in the range
$0 \le c \le 1$ and when completely satisfied, $c = 1$.

\subsubsection{V-measure score}
V-measure is a weighted harmonic mean of homogeneity and completeness, and
is defined by \citeauthor{rosenberg07} as:

\[
    V_{\beta} = \frac{(1 + \beta) * h * c}{(\beta * h) + c}
\]

We weigh homogeneity and completeness equally in our application, thus setting
$\beta = 1$.

\section{Sequence alignment}
The problem of aligning sequences with one another originally arose in the area
of bioinformatics where a need to find similarities in long chains of amino
acids existed. One of the first methods that solved this problem was the
Needleman-Wunsch algorithm \citep{needleman70}. The algorithm finds the maximal
number of matching symbols between two sequences, allowing for gaps to be
inserted into either sequence. This is done by tracing a path through an
alignment matrix where each element represents a possible matching of symbols
between the sequences.

Originally, the path was built only with respect to the number of matching
identical symbols and not to the number of mismatches or gaps. This has since
been improved upon to include pairwise scores for symbol matching and penalties
for gaps. The different scores used when matching symbols are typically given in
a similarity matrix. An example of such a matrix for a small alphabet of
symbols is seen in Figure~\ref{fig:simmatrix}.

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \begin{tabular}{| c | c | c | c | c |}
            \hline
              &  A &  C & G &  T \\ \hline
            A &  2 &  1 & 0 &  1 \\ \hline
            C &  1 &  2 & 1 & -1 \\ \hline
            G &  0 &  1 & 2 &  1 \\ \hline
            T &  1 & -1 & 1 &  2 \\ \hline
        \end{tabular}
        \caption{The similarity matrix for the alphabet.}
        \label{fig:simmatrix}
    \end{subfigure}
    \quad
    \begin{subfigure}[b]{0.55\textwidth}
        \begin{tabular}{| c | c | c | c | c | c | c | c | c |}
            \hline
            & & A & T & T & C & G & C & T \\ \hline
            &\cellcolor[gray]{0.9}0 & -1 & -2 & -3 & -4 & -5 & -6 & -7 \\
            \hline
            A & -1 &\cellcolor[gray]{0.9}2 & 1 & 0 & -1 & -2 & -3 & -4 \\
            \hline
            C & -2 &\cellcolor[gray]{0.9}1 & 1 & 0 & 2 & 1 & 0 & -1 \\ \hline
            T & -3 & 0 &\cellcolor[gray]{0.9}3 & 3 & 2 & 3 & 2 & 2 \\ \hline
            T & -4 & -1 & 2 &\cellcolor[gray]{0.9}5 &\cellcolor[gray]{0.9}4 & 3
            & 2 & 4 \\ \hline
            G & -5 & -2 & 1 & 4 & 6 &\cellcolor[gray]{0.9}6 & 5 & 4 \\ \hline
            C & -6 & -3 & 0 & 3 & 6 & 7 &\cellcolor[gray]{0.9}8 &
            \cellcolor[gray]{0.9}7 \\ \hline
        \end{tabular}
        \caption{The alignment matrix for the two sequences
            $S = \texttt{ATTCGCT}$ and $T = \texttt{ACTTGC}$.}
        \label{fig:nwmatrix}
    \end{subfigure}
    \caption{An example of the Needleman-Wunsch algorithm run on two sequences
    constructed from the alphabet $\{A, C, G, T\}$ with a gap penalty $d = -1$.
    The similarity matrix used can be seen in (\subref{fig:simmatrix}) and the
    resulting alignments is given by the gray elements in
    (\subref{fig:nwmatrix}).}
    \label{fig:nwtables}
\end{figure}

Given two sequences, $S$ and $T$, of length $m$ and $n$ respectively, a scoring
matrix $C$ and a gap penalty $d$ the algorithm does the following:

\begin{enumerate}
    \item Construct an $(n + 1) \times (m + 1)$ alignment matrix $A$
    \item Fill the first row of $A$ with gap penalties: $a_{0,j} \gets j \cdot
        d$ where $0 \le j \le m$
    \item Fill the first column of $A$ with gap penalties: $a_{i,0} \gets i
        \cdot d$ where $0 \le i \le n$
    \item Fill the rest of $A$ by doing one of the following actions for each
        element $a_{i,j}$:
        \begin{itemize}
            \item Match: $a_{i,j} \gets a_{i - 1, j - 1} + C_{S_j,T_i}$
            \item Delete: $a_{i,j} \gets a_{i - 1, j} + d$
            \item Insert: $a_{i,j} \gets a_{i, j - 1} + d$
        \end{itemize}
        The action that is chosen is the one that results in the largest score
        where, in the case of a tie, match is given priority. The priority of
        delete and insert can be chosen arbitrarily.
    \item Backtrack through the matrix from $a_{n,m}$ to build the alignments
        in reverse order. Choose the path that was used to construct the score
        at the current element. The algorithm finishes when element $a_{0,0}$
        is reached.
\end{enumerate}

An example of applying the algorithm can be seen in Table~\ref{fig:nwtables}
and the resulting alignments in Figure~\ref{fig:align}.

\begin{figure}[h]
    \centering
    \texttt{A-TTCGCT\\ACTT-GC-}
    \captionsetup{width=0.8\textwidth}
    \caption{The resulting alignment of the sequences \texttt{ATTCGCT} and
    \texttt{ACTTGC}.}
    \label{fig:align}
\end{figure}

\section{Simple linear regression}
In statistics, a response variable $Y_i$ that is linearly dependent on an
explanatory variable $X_i$ can be modeled with a simple linear regression
model. The relationship between the two variables are then

\begin{equation}
    Y_i = \alpha + \beta X_i + \varepsilon_i
    \label{eq:slr}
\end{equation}

where $\alpha$ and $\beta$ are the \emph{regression coefficients} and
$\varepsilon_i$ is the error term. Given a set of data
$\displaystyle \{Y_i, X_i\}_1^n$ the regression coefficients can be estimated
through linear regression. This process is often called fitting as it can be
seen as trying to find the model with the best fit for the data. There are
different methods to do this as there is no consistent measure for what is the
best fit.

\subsection{Ordinary least squares}
Ordinary least squares (OLS) is a method for estimating the regression
coefficients in Equation~\ref{eq:slr}. It does this by minimizing the sum of
squared residuals of the simple linear regression model. This can be formulated
as minimizing the following cost function.

\begin{equation}
    C(\alpha, \beta) = \sum_{i=1}^n(Y_i - \alpha - \beta X_i)^2
\end{equation}

The result of this is a model that minimizes the residuals globally across all
samples. This intrinsically makes the method \emph{non-robust}, i.e. sensitive
to outliers.

\subsection{RANSAC}
\label{sec:ransac}

Random Sample Consensus (RANSAC) \citep{fischler81} is a \emph{robust} method
for fitting a dataset to a model and was originally intended for image
analysis. The robustness of the method allows it to avoid the influence of
outliers in the model. Given a dataset $D$, a model $M$, a set of inliers $I$
and two confidence parameters, $\delta$ and $\gamma$, it works the following
way:

\begin{enumerate}
    \item Fit $M$ to $I$
    \item Find the samples that are within $\delta$ distance from $M$ and add
        those samples to $I$
    \item Continue if $|I|$ is at least $\gamma$
    \item Fit $M$ to $I$
    \item Evaluate $M$ from the residual of $I$
\end{enumerate}

One problem with this approach is that an initial set of inliers is required.
In most cases this is not available so we need to guess which samples might be
inliers. A simple solution to this is to take $n$ random samples and assume
that they are inliers. This only works if the assumption is actually true and
thus the resulting algorithm will be \emph{non-deterministic}, meaning that it
only produces a good fit with a certain probability. The algorithm is normally
run $k$ number of iterations until the probability of success is sufficiently
large. Pseudocode for RANSAC can be seen in Algorithm~\ref{alg:ransac}.

{
    \fontsize{10}{12}
    \selectfont
    \begin{algorithm}[t]
        \DontPrintSemicolon
        \BlankLine
        \KwIn{$D$, $M$, $n$, $\delta$, $\gamma$, $k$}
        \KwOut{$P_b$}
        \BlankLine
        $P_b \gets \varnothing$\;
        $\varepsilon_b \gets \infty$\;
        \For{$k$}{
            $I \gets n$ number of randomly chosen samples\;
            fit $M$ to $I$\;
            $I_m \gets$ samples where distance from $M \leq \delta$\;
            $I \gets I \cup I_m$\;
            \If{$|I| \geq \gamma$}{
                fit $M$ to $I$\;
                $\varepsilon \gets$ deviation from $M$ for all $I$\;
                \If{$\varepsilon < \varepsilon_b$}{
                    $P_b \gets$ parameters in $M$\;
                    $\varepsilon_b \gets \varepsilon$\;
                }
            }
        }
        \Return{$P_b$}
        \BlankLine
        \caption{RANSAC}
        \label{alg:ransac}
    \end{algorithm}
}

The model that RANSAC operates on can be any kind of mathematical model,
therefore when handling two-dimensional linear data the simple linear
regression model given in Equation~\ref{eq:slr} can be used. This gives us a
method that solves the problem of simple linear regression but that is also
insensitive to outliers. A comparison between OLS and RANSAC on a dataset where
some samples have been replaced by random noise can be seen in
Figure~\ref{fig:ransac}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{img/ransac}
    \captionsetup{width=0.8\textwidth}
    \caption{A comparison between OLS and RANSAC on a dataset containing
    outliers.}
    \label{fig:ransac}
\end{figure}

\chapter{Method}

\section{Approach}
A protocol is generally made up of a predefined set of message types. A
message type may define if the message is a request or response. If so, it is
fairly easy to determine which parts of the message that defines this. One can
simply look for values in a message that are dependent on the direction of the
message. However, most protocols are not limited to being classified only as
requests and responses. Protocols often define sets of control message types
which change the state of the protocol. Some protocols do not even follow the
request/response model, which is the case for many peer-to-peer protocols. Our
method tries to overcome these obstacles.

\subsubsection{Type inference}
In order to thoroughly analyze a protocol, the first step is to infer the
different message types. This is necessary for us to perform further analysis
of the different protocol states and allows for type-specific field analysis.
The message type is commonly specified by some flag or a combination of flags.
A type flag may be a byte, a subset of bits within a byte, or spread out over
multiple bytes where some may define subtypes.

Messages that have the same value in their type flags are likely to have a
distinguishable structure which differs from messages with other values in the
same type flags. Some message types may introduce fields which are not present
in other message types, and the fields which are global for the protocol are
more likely to be identical within a type. A message that is used for binary
data transmission is often longer than a control message.

We group messages that are similar using cluster analysis. We then move on to
analyzing the clustered messages in an effort to discover the bytes which we
rank the most probable to be responsible for the clustering we have found, and
label these as \emph{type distinguishers}, i.e. the bytes that distinguishes
between different types. Once these bytes have been discovered, we may regroup
the messages based on the values of these bytes.

\subsubsection{Field inference}
From the grouping acquired by the type inference, we are able to analyze the
type specific properties of the messages. We focus on determining the field
structure for each type, and make an effort to determine the semantics of each
field. From the byte value distribution at a certain byte range, we try to
classify each field as one of our primitive field types: constant, flag,
uniform, number, incremental or length. We perform this analysis in global
scope, cluster scope, stream scope and connection scope. We define the
global scope to cover the fields that all messages have in common, which
usually the corresponds to the protocol header. The cluster scope covers fields
that are specific for that type. The connection scope is a subscope to the
global scope, and cover fields that are constant or incremental within a
connection. The same goes for the stream scope but only in one direction.

\subsubsection{State inference}
Once we have knowledge of the message types we may analyze the order in which
messages of different types are sent. From this analysis we build a state
diagram for the protocol. From this state diagram one may get an overview of
the order in which messages are sent and the probability for subsequent
messages being a specific type.

\section{Type inference}
\label{sec:type_inf}
When inferring the message types, we perform a two-pass clustering. In the
first pass, we use a clustering algorithm in order to statistically group
messages that are similar according to the features we define. In the second
pass, we utilize the assumption that there exists one or more bytes which
controls the message type.

\subsection{Initial clustering}
\label{sec:init_clust}
The first step when performing cluster analysis is to define a set of features.
The features which we have chosen are based on the byte value distribution for
all messages.

We define a matrix $P$ where each element $p_{i,j}$ is the observed probability
that the byte with the decimal value $j$ is present at the offset $i$ for all
packets. Using $P$ we create the feature vector $\{f_1, f_2, ..., f_n\}$ for
every message $m$ where $n$ is the length of the longest message. We
define each feature as $f_i = p_{i, m_i}$ within the length of $m$ and $f_i = 0$
otherwise.

Informally, this means that we create a feature vector consisting of $n$
features for each message. Each element $f_i$ in the feature vector is the
probability that the byte value $m_i$ occurs at offset $i$.

The reason for having probability-based features is that density-based
clustering algorithms such as OPTICS rely on distance measures. If we were to
take the actual byte values at each offset as features, the difference between
two byte values would represent a distance between the samples. In a flag byte
this would introduce a great distance between a message when the most
significant bit is set and when it is not set. Since nominal features are not
an option due to difficulties with high dimensionality, the probability
measure is a compromise to having nominal features.

% Latex does not know how to split parametrized -> badboxes if splitpoint
% not manually defined.
Once the features have been generated, we apply a PCA transformation
param-etrized to capture at least $80\%$ of the variance. This reduces the
dimensionality of our feature vectors while conserving the most significant
variance. We use these new feature vectors as input to OPTICS.

OPTICS works well for our application since it is insensitive to the parameters
selected and that it is a suitable clustering algorithm for when the number of
classes are unknown. We require an estimated $MinPts$ parameter while the distance
parameter defaults to $\varepsilon = \infty$.

The resulting clustering from OPTICS may have far more clusters than the number
of real types in the protocol. This is expected, since messages that are
identical on large partitions of the data will induce a low distance when
running the OPTICS algorithm, which in turn will result in high-density
areas within the real types. Having more clusters than the number of real
types is not a problem however. In the next step of the clustering, we make
use of the fact that the clusters are mostly homogeneous, i.e. each cluster mainly
contains messages of a single real type.

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\textwidth]{img/num_optics}
        \caption{Number of clusters found by OPTICS for different input sizes.}
        \label{fig:num_optics}
    \end{subfigure}
    \quad
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\textwidth]{img/metrics_optics}
        \caption{Clustering metrics for our OPTICS clustering for different
            input sizes.}
        \label{fig:metrics_optics}
    \end{subfigure}
    \caption{Performance of the OPTICS algorithm on SMB messages with respect
        to input size.}
    \label{fig:optics_res}
\end{figure}

Figure~\ref{fig:optics_res} visualizes how OPTICS performs for various input
sizes of SMB messages. The number of messages needed in order to find a good
clustering is fairly small. However, a large input size is often needed in
order to capture as many message types as possible.

\subsection{Type distinguishers}
Once we have obtained the OPTICS clustering our goal is to reduce the number of
clusters to match the actual number of real types. We do this under the
assumption that there is some byte or a number of bytes that define the type of
each message. We call these bytes \emph{type distinguishers}.

Many protocols define an explicit type flag in the form of a single byte. The
type distinguishers which fit our definition may consist of a number of bytes.
When combined, these bytes give a reasonable indication of what messages with
high similarity have in common. In order to find these bytes we use an approach
similar to the recursive clustering described by \citeauthor{cui07} in
Discoverer. Our approach is as follows:

\begin{enumerate}
    \item For every byte offset from the beginning of all messages up to a
    certain limit we calculate how many values the byte takes. This is equivalent
    to how many real types that would be present if that byte was a real
    type flag.

    We introduce two criteria when considering a byte as a type distinguisher.
    The first criterion is $MaxNumTypes$ which decides the maximum number of
    types the protocol is assumed to have. The second criterion is
    $MaxTypeRatio$ which decides the maximum ratio between the number of packets
    with a certain value for the considered byte and the total number of
    packets. If $MaxTypeRatio = 0.6$, it prevents a byte from being considered
    a possible type distinguisher if it assumes the same value in more than
    $60\%$ of all messages.

    \item Now that we have the possible type distinguishers we rank the type
    distinguishers based on its completeness score. For each possible type
    distinguisher byte we group the messages depending on the value which the
    byte takes. This gives us a new clustering. We measure the quality of each
    type distinguisher by comparing its clustering to the OPTICS clustering.
    This is achieved using the completeness score as our metric and the OPTICS
    clustering as our truth.

    \item Once our type distinguisher bytes have been scored, we select the bytes
    with the highest completeness score. For each newly selected byte, we calculate
    how many clusters would be yielded if we were to group all messages based on
    the distinct values for the combination of all selected bytes. This process
    goes on until we reach the point where we can no longer select any more
    type distinguisher bytes without exceeding $MaxNumTypes$.
\end{enumerate}

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\textwidth]{img/num_td}
        \caption{Number of clusters found by our type distinguisher clustering
            for different input sizes.}
        \label{fig:num_td}
    \end{subfigure}
    \quad
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\textwidth]{img/metrics_td}
        \caption{Clustering metrics for our type distinguisher clustering for
            different input sizes.}
        \label{fig:metrics_td}
    \end{subfigure}
    \caption{Performance of the OPTICS algorithm followed by the type
        distinguisher clustering on SMB messages with respect to input size.}
    \label{fig:td_res}
\end{figure}

In Figure~\ref{fig:td_res} the performance of our type distinguisher clustering
on SMB messages is visualized. This step in our method gives an obvious
enhancement compared to the results from the OPTICS clustering.

\section{Field inference}
\label{sec:field_inf}
With our type inference complete we now have a number of clusters that contain
similar messages. The next step in inferring the structure of the protocol is
to identify field boundaries and their contents. This is a very complex
problem as there can be as many fields as there are bits in the messages and
the boundaries may lie in between any pair of adjacent bits. The number of
possible combinations of fields can therefore easily become too large to
handle. Fortunately, most protocols are not designed to utilize each and every
bit and instead focus on being extensible, thus there are some common design
principles that may be exploited.

\subsection{Simplifications}
\label{sec:simplifications}
First of all we restrict where boundaries may lie. We require fields to be $n$
bytes in length where $n$ is a power of two and also that they must be aligned.
We use the definition of \emph{$n$-byte alignment} which means that the offset
at which the data is located must be a multiple of $n$. That means that a four
byte field may be located at offset 0, 4 or 12 but not at offset 5 or 10. Most
protocols follow these principle for optimization reasons as a received packet
will reside in a byte buffer in memory and accessing misaligned data can be
expensive; see e.g. Assembly/Compiler Coding Rule 46 in \citetalias{intel12}
for guidelines on memory alignment.

Another principle that is often used is that messages start with a
\emph{header}, a collection of fields of fixed length that is located before
the data part of the message. Most protocols need a header if they send
messages of different types or messages with variable length, or both. In those
cases the header will include at least one type flag or length field
respectively. A type flag can be a single bit or a collection of bits that
indicate what type the current message is so that it can be parsed correctly. A
length field is a numerical field that contains metadata on how long a part of
the message is, also for parsing purposes. As the header itself usually does
not contain any variable length fields no preprocessing is needed to identify
them, however for the rest of the message, which is usually type specific, the
data needs to be aligned.

\subsection{Non-mutual sequence alignment}
\label{sec:nonmutualalign}

\begin{figure}[t]
    \begin{subfigure}[b]{0.55\textwidth}
        \centering
        \begin{tabular}{| c | c | c | c | c | c | c | c | c |}
            \hline
            & & A & T & T & C & G & C & T \\ \hline
            &\cellcolor[gray]{0.9}0 & -1 & -2 & -3 & -4 & -5 & -6 & -7 \\
            \hline
            A & &\cellcolor[gray]{0.9}2 & 1 & 0 & -1 & -2 & -3 & -4 \\
            \hline
            C & & &\cellcolor[gray]{0.9}1 & 0 & 2 & 1 & 0 & -1 \\ \hline
            T & & & &\cellcolor[gray]{0.9}3 & 2 & 3 & 2 & 2 \\ \hline
            T & & & & &\cellcolor[gray]{0.9}2 & 3 & 2 & 4 \\ \hline
            G & & & & & &\cellcolor[gray]{0.9}4 & 4 & 3 \\ \hline
            C & & & & & & &\cellcolor[gray]{0.9}6 &
            \cellcolor[gray]{0.9}5 \\ \hline
        \end{tabular}
        \caption{The alignment matrix for sequences using non-mutual sequence
        alignment.}
        \label{fig:nonmutmatrix}
    \end{subfigure}
    \quad
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \texttt{ATTCGCT\\ACTTGC-}
        \caption{The resulting alignment of the sequences.}
        \label{fig:nonmutalign}
    \end{subfigure}
    \caption{An example of our non-mutual sequence alignment algorithm run on
    the two sequences \texttt{ATTCGCT} and \texttt{ACTTGC} using the same
    parameters as in Figure~\ref{fig:nwtables}.}
    \label{fig:nonmuttables}
\end{figure}

For our purposes we have introduced the term \emph{mutual} sequence alignment
to refer to traditional sequence alignment via the Needleman-Wunsch algorithm.
Furthermore, we have developed a modified version of the Needleman-Wunsch
algorithm that performs \emph{non-mutual} sequence alignment. The difference
between the two algorithms is that the non-mutual version only allows gap
insertion into one of the two sequences. This restriction only works when the
gap insertion is allowed for shorter one of the two sequences; otherwise the
algorithm will never be able to reach the top left element during the
backtracking. For simplicity let us assume that the longer sequence will always
be supplied in $S$, therefore gaps may only be inserted into $T$. The
non-mutual property is achieved by the following modifications:

\begin{enumerate}
    \item After the first row has been filled with gap penalties, fill the
        diagonal with matches: $a_{j,j} \gets a_{j - 1, j - 1} + C_{S_j,T_j}$
        where $1 \le j \le n$
    \item Do not fill the first column with gap penalties.
    \item During the filling phase, only allow the match and insert actions,
        i.e. remove the delete action, and only fill the upper part of the
        matrix (above the diagonal).
\end{enumerate}

If we run this algorithm on the same example as in Figure~\ref{fig:nwtables}
we get the result shown in Figure~\ref{fig:nonmuttables}..

\subsection{Preprocessing}
Using this method of non-mutual sequence alignment we align each message within
each cluster with the longest message in that cluster. This way we get an
effective method for doing multiple sequence alignment of all messages in a
cluster in a linear number of alignments. To model that variable length data
pushes the next field boundary forward we also enforce gap insertion from the
right instead of from the left by simply reversing the messages before aligning
them and then reversing them back again after the alignment is complete. During
the alignment we also use a somewhat complex $256 \times 256$ scoring matrix to
get a good result. It is constructed to provide the following scores:

\begin{itemize}
    \item Identical byte values: 2
    \item Alphanumeric ASCII characters: 2
    \item Non-alphanumeric ASCII characters: 1
    \item Numerical distance $\le 10$: 1
    \item Numerical distance $\le 20$: 0
    \item Anything else: -1
\end{itemize}

In addition to aligning the messages for the cluster scope of the analysis we
also group messages by stream and connection. These groups are then used when
attempting to find fields that are constant in the stream or connection scope.
This is accomplished by looking at each message at the IP level and grouping
them together depending on their combination of addresses and ports. After this
preprocessing of the messages has been done we are now ready to start
classifying fields.

\subsection{Field classification}
Now that we have limited our possible field locations and prepared our data for
searching in different scopes, we try to classify our data into a predefined
set of field types. This is done by iterating through all possible aligned
locations of a set of field sizes, typically $\{2^n~|~0 \le n \le 2\}$, and
testing if the data at that location may belong to any of the predefined field
types. We will then get a number of different possible types for each byte
position, possibly of different sizes that might be overlapping. This is our
estimate of the field types. To get the most likely candidate we can then give
precedence to different sizes and types, favoring larger sizes and more complex
types. The field types we have identified are the following:

\begin{itemize}
    \item Constant
    \item Flag
    \item Uniform
    \item Number
    \item Incremental
    \item Length
\end{itemize}

These field types have been chosen because they are commonly used in the
construction of protocols and because they exhibit some degree of
identifiability. For the top three types we use simple techniques on the
distributions of the individual bytes of all the messages in the current scope
while for the bottom three we need to resort to more advanced analytical
techniques on the data of each individual message.

\newpage

\subsection{Constant fields}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{img/const_one}
    \captionsetup{width=0.8\textwidth}
    \caption{Byte value distribution of the 6th byte in the DNS header. This
        byte is the least significant byte of the two-byte query count field.
        In our dump its value is constantly one.}
    \label{fig:const_one}
\end{figure}

We define a constant field as a field that only assumes a single value in all
the messages in the current scope. By looking at the byte value probability
distributions, we classify an offset $i$ as constant if $p_{i,v} = 1$ holds for
some $v$, $0 \le v \le 255$. $p_{i,v}$ is an element in the probability
matrix $P$ which is defined in Section~\ref{sec:init_clust}.

Constant fields may be interpreted as one of the following:
\begin{itemize}
    \item A true protocol constant. These are fields that are defined to be
        constant in the protocol specification. An example of such a field is
        the protocol field in SMB, which is a four-byte constant field with
        the value \verb+0xff534d42+.
    \item A reserved field. Reserved fields are specified in case there is need
        for future additional information in a fixed-sized header. Reserved
        fields are commonly set to zero.
    \item A field which is constant in the current scope. For example, if a dump
        only contains a single connection, then a session ID field may be
        interpreted as a constant.
\end{itemize}

\newpage

\subsection{Flag fields}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{img/flag}
    \captionsetup{width=0.8\textwidth}
    \caption{Byte value distribution for the 3rd byte in the DNS header.
    From this byte it may be determined if the message is a query or a
    response.}
    \label{fig:flag}
\end{figure}

A flag field is a field that defines some property of the message. Typically,
it assumes a very limited number of values and may be used for representing
a message type, an error code, a status code and so on.

Flag fields may be interpreted as real numbers where each number represents a numbered entity, such as a
type identifier. It may also represent a set of binary values where each bit
is a flag itself, and is interpreted using a bitmask. An example of such a
field is the DNS flags field. The flag field in DNS is two bytes long, where
the first bit of the field indicates if it is a query (0) or response (1).

The first byte of the DNS flag field is depicted in Figure~\ref{fig:flag}.
The amount of queries and responses are relatively equally distributed. The
bar at byte value 0 = \underline{0}0000000 represent the queries. The bar at
byte value 128 = \underline{1}0000000 represent the responses.

When classifying flag fields, we place constraints on the amount of values that
a certain byte may assume. A field is classified as a flag if the total amount
of unique values the field assumes is greater than one and less than some upper
bound.

\newpage

\subsection{Uniform fields}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{img/uniform}
    \captionsetup{width=0.8\textwidth}
    \caption{Byte value distribution for the first byte of the DNS header.
        This byte is the first of a two-byte ID field.}
    \label{fig:uniform}
\end{figure}

A pattern we observed while inspecting the byte value distributions for
various fields was that some fields were uniformly distributed. The real
types of these fields are normally some randomly generated value, such as
a generated session identifier or a nonce.

When classifying fields as uniform, we compare the distribution of the byte
values to the uniform probability distribution for each byte. If we assume the
observed byte value is a random variable $X$ and the expected value the scalar
$x$ then the uniform probability distribution for a byte is

\[
    Pr[X = x] = \left\{
        \begin{array}{l l}
            \frac{1}{256} & \quad 0 \le x \le 255 \\
            0 & \quad \text{otherwise}
        \end{array}
    \right.
\]

If the sum of the deviations from this distribution is below a certain
threshold then the byte is classified as being uniform. If all bytes are
classified as uniform then the field is classified as uniform as well.

In Figure~\ref{fig:uniform} the value distribution for the first byte of the
DNS header is displayed. This byte is the first byte of a two-byte identifier.
This ID is generated by the client and used by the DNS server in order to
identify which query it issues a response to. The black line represents the
uniform distribution; a horizontal line at $\frac{1}{256}$.

\newpage

\subsection{Number fields}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{img/number}
    \captionsetup{width=0.8\textwidth}
    \caption{A fitted normal distribution to the distribution of values of the
    13th byte in DNS.}
    \label{fig:number}
\end{figure}

While every field in a binary protocol actually contains numbers, some exhibit
a more natural distribution of these numbers than others. We define number
fields as those that can be closely fitted to a normal distribution with
constraints on the standard deviation. We set these constraints so that the
distribution will not collide with our definitions for constant or uniform
fields. This captures the behavior of a random variable $X$ with mean $\mu$ and
standard deviation $\sigma$. Typically, values that represent a count behave
this way, e.g. the QDCOUNT or ANCOUNT fields of the DNS protocols that refer to
how many questions or answers respectively the message contains. This behavior
can also been seen in length fields as they represent a byte count. An example
of such a byte count can be seen in Figure~\ref{fig:number}.

\newpage

\subsection{Incremental fields}
Some protocols contain fields where the value increases for each message in the
scope. It can be a sequence number for each packet or a timestamp of when a
certain action was performed. We define these as incremental fields. These can
be identified by looking at all the messages within a scope and counting how
many of them contain a value that is greater than the previous encountered
value. We then compare this count to the total number of messages in the scope
and if it is above a certain threshold we accept it as being incremental. The
threshold value is used to allow for some amount of wrap-around of the values
in the field.

\subsection{Length fields}
\label{sec:length}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{img/length}
    \captionsetup{width=0.8\textwidth}
    \caption{RANSAC fit of the WC field in the SMB protocol. The resulting
    parameters are $\alpha = 39$ and $\beta = 2$.}
    \label{fig:length}
\end{figure}

Length fields require the most analysis to identify but also gives a very
confident estimation because of the depth of the analysis. As mentioned in
Section~\ref{sec:simplifications}, length fields relate to how long a part of
the message is. This part can be the entire message, the part following the
header or just the next field. For a single message all these types of length
fields can be found, as long as there is only one variable length part in the
message. For the first case the length of the message will be a multiple of the
numerical value in the field. Likewise, in the two last cases, the length will
be a multiple of the numerical value in the field plus an offset equal to the
length of the parts not covered by the field.

Unfortunately, if a message contains multiple parts of variable length the
problem becomes more difficult and cannot be solved by looking at individual
messages anymore. There is however a better way to approach this problem. We
have already seen that the value of the length field multiplied by some scalar
plus an offset (which may be zero) is equal to the length of the message. If
$x$ is the value of the field, $y$ is the length of the message, $\alpha$ is the
offset and $\beta$ is the multiple then this can be written as

\[
    y = \alpha + \beta x
    \label{eq:linlength}
\]

which means that this can be modeled with a linear model. We can then attempt
to solve this with simple linear regression for several messages at once. If
the samples are diverse enough there should be a linear relationship between
the minimum size of all messages with a specific field value and the value
itself. This represents the case of the other variable fields being minimized.
The samples that are not of minimal size for the specific field value will then
be seen as noise. Because of this noise, which normally is the dominant part of
all samples, we need a robust method for fitting our linear model. We therefore
use the RANSAC method as described in Section~\ref{sec:ransac} for this
purpose. A real world example of this for the SMB protocol can be seen in
Figure~\ref{fig:length}.

One added advantage of using the RANSAC method is that it also handles other
situations where noise appears. This can be if the protocol handles its own
fragmentation or some other form of imperfect parsing that affects the length
of the messages.

\section{State inference}
Protocols may be seen as having an internal state that represents which
messages are valid at a certain point in the communication. Some handshake
phase between the peers is common when a session is initialized. There might
also be some teardown phase, and phases for transmitting data etc.
Inferring the states of the protocol is an important task when trying to
figure out the semantics of the protocol. Modeling the protocol states as
a state diagram provides a readable representation of the inner workings
of the protocol, and might simplify the analysis of the fields of the
different message types.

When a reasonable partitioning into types has been achieved, constructing the
protocol state diagram is trivial. We simply partition the set of messages
into connections, where connections are tuples of client and server sockets.
From each connection, we define the messages in that connection as a change
of state. Each state has a set of probabilities for transitioning to
all other states we have defined.

We represent the states as two $n \times n$-matrices, $C$ and $S$, where
$n$ is the number of types, which will be the same as the number of states.
Each entry $c_{i,j}$ or $s_{i,j}$ in the matrices defines the number of times
we observe a transition by the client or server respectively from state $i$ to
state $j$. We define the client as the peer that initiates the connection. Once
we have our two matrices, we may calculate the probabilities for the state
transitions by normalizing each row.

A state diagram of all protocol states and transitions is not always
practical however. Some protocols allow certain message types to be sent from
all states, resulting in a cluttered state diagram. We handle this problem by
allowing the user to define the depth for which the analysis is performed.
If the depth is set to five, we only analyze the states that the protocol can
reach in five transitions from the starting state. From this restricted state
diagram a handshake procedure will be much more readable.

Figure~\ref{fig:states} shows state diagrams generated from DNS and SMB data.
In the figure light-gray edges represent messages sent by the client while dark-gray
edges represent messages sent by the server. Table~\ref{tab:smb} lists the
true types of the messages in a left-to-right order with respect to the state
diagram.

\begin{table}
    \centering
    \captionsetup{width=0.8\textwidth}
    \caption{SMB states.}
    \begin{tabular}{ | l | l |}
        \hline
        \textbf{State}&\textbf{Type}\\ \hline
        37          & Negotiate Protocol Request    \\ \hline
        15          & Negotiate Protocol Response   \\ \hline
        75          & Session Setup AndX Request    \\ \hline
        48          & Session Setup AndX Response   \\ \hline
        14          & Logoff AndX Request           \\ \hline
        50          & Tree Connect AndX Request     \\ \hline
        20          & NT Create AndX Request        \\ \hline
        24          & Delete Request                \\ \hline
    \end{tabular}
    \label{tab:smb}
\end{table}

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{img/dnsstate}
        \caption{State diagram for the DNS protocol generated from our
        method with $MaxNumTypes = 3$. State 0 and 1 represents the last message
        being a query and a response respectively.}
        \label{fig:dnsstate}
    \end{subfigure}
    \quad
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{img/smbstate}
        \caption{State diagram for the SMB protocol with a depth of five
        transitions. A handshaking procedure is clearly visible in the first
        three states.}
        \label{fig:smbstate}
    \end{subfigure}
    \caption{State diagrams generated by our state inference. The left-most
        dot represents the state before any communication has begun. The
        right-most double-circle represents the terminal state.}
    \label{fig:states}
\end{figure}

\chapter{Results}
We measure the performance of our method by comparing the results from the
type inference and field inference to annotated data from a variety of network
captures for binary protocols.

A common property for all the datasets we have analyzed is that they do
not capture the entire protocol they contain. If some field in the protocol is
constant within the dump we will have difficulties classifying it correctly.
Such fields may be flags that denotes some error code or some uncommon state of
the protocol.

These difficulties emphasize the importance of diverse network captures.

\section{Datasets}
We have chosen five different data sets for our performance analysis. Each
data set contains messages from a single protocol. All the protocols vary
in complexity, header size and usage area. The protocols are the following:

\begin{itemize}
    \item \textbf{SMB}: \emph{Server Message Block}, a protocol mainly used
        in Microsoft Windows for file-sharing, printer-sharing and access
        control. The protocol has a wide variety of types and a fairly complex
        header. The protocol messages are generally wrapped in a
        NetBIOS-message.
    \item \textbf{DNS}: \emph{Domain Name System}, a system for resolving
        IP-addresses from domain names. The DNS header is simple, but its
        payload contains a variable number of fields. Each of these fields
        denote a subdomain or domain to some URL.
    \item \textbf{TFTP}: \emph{Trivial File Transfer Protocol}, a very simple
        file transfer protocol. It defines a very limited number of types
        and a very simple header.
    \item \textbf{RTP}: \emph{Real-Time Transport Protocol}, a protocol for
        streaming audio and video. Its header is fairly simple, but in our
        dump it has no diversity in the payload type. This makes type
        inference difficult.
    \item \textbf{DHCP}: \emph{Dynamic Host Configuration Protocol}, a
        protocol for obtaining network configuration information such as
        IP addresses in a network. The protocol is an extension to the
        \emph{Bootstrap Protocol}, BOOTP. Combined with BOOTP, it has a large
        header and a number of extension headers to BOOTP.
\end{itemize}

In Table~\ref{tab:datasets} we list some of the properties for our different
datasets.

\begin{table}[h]
    \centering
    \captionsetup{width=0.8\textwidth}
    \caption{Dataset properties.}
    \begin{tabular}{| l | r | r | r |}
        \hline
        \textbf{Protocol}&\textbf{\# Messages}&\textbf{\# Connections}&\textbf{\# Types}\\ \hline
        DNS         & 30628         & 10935         & 6         \\ \hline
        SMB         & 27906         & 212           & 93        \\ \hline
        TFTP        & 6492          & 6             & 3         \\ \hline
        RTP         & 20179         & 5             & 1         \\ \hline
        DHCP        & 13014         & 376           & 11        \\ \hline
    \end{tabular}
    \label{tab:datasets}
\end{table}

\section{Clustering performance}
\label{sec:clustperf}
We measure the performance of our clustering using the metrics described in
Section~\ref{sec:metrics}: homogeneity, completeness and V-measure. The truth we
use when calculating the metrics is based on the protocol definitions in
Wireshark.

In Table~\ref{tab:clusterparams}, we list the parameters we use on the
different datasets. The initial clustering, which uses OPTICS, uses the
$Limit$ and $MinPts$ parameters. The type distinguisher clustering uses
the results from the OPTICS clustering and the $MaxNumTypes$ parameter. All of
these parameters are rather insensitive to change.
% Metrics: homogeneity, completeness, V-measure
% Improvement over dump size; all metrics
%   Before type distinguisher clustering
%   After type distringuisher clustering

\begin{table}[h]
    \centering
    \captionsetup{width=0.8\textwidth}
    \caption{Clustering parameters.}
    \begin{tabular}{| l | r | r | r |}
        \hline
        \textbf{Protocol}&\textbf{Limit}&\textbf{MinPts}&\textbf{MaxNumTypes} \\ \hline
        DNS & 200 & 200 & 10 \\ \hline
        SMB & 200 & 50 & 100 \\ \hline
        TFTP & 200 & 50 & 10 \\ \hline
        RTP & 200 & 200 & 10 \\ \hline
        DHCP & 400 & 50 & 20 \\ \hline
    \end{tabular}
    \label{tab:clusterparams}
\end{table}

Table~\ref{tab:initclusterresults} shows the performance of our OPTICS
implementation. Like the clustering in Discoverer by \citeauthor{cui07} we
are most concerned with achieving high homogeneity. A high homogeneity assures
that each cluster mainly contains messages from one true type. A high
completeness is not as important in our initial clustering. A high completeness
gives us a small number of clusters for each true type, ideally a single
cluster.

The performance for the RTP and DHCP data sets are inferior to the other
protocols. OPTICS has bad performance on RTP since our dump only contains messages
from a single message type. Instead of clustering on type-specific data, it
proceeds to cluster the messages on the protocol payload. Since the payload
data is very specific for each connection, each cluster mainly contains
messages from a single connection. The DHCP dump is difficult for the
clustering due to the size of the header. The BOOTP header is over 200 bytes
long. All DHCP-specific data is located as extensions to BOOTP, and these
extensions are located at the end of the header.

\begin{table}[h]
    \centering
    \captionsetup{width=0.8\textwidth}
    \caption{Clustering performance before type distinguisher clustering.}
    \begin{tabular}{| l | r | r | r | r | r |}
        \hline
        \textbf{Protocol}&\textbf{Completeness}&\textbf{Homogenity}&\textbf{V-measure} \\ \hline
        DNS & 0.1982 & 0.9550 & 0.3282 \\ \hline
        SMB & 0.8480 & 0.8632 & 0.8555 \\ \hline
        TFTP & 0.3015 & 0.9449 & 0.4571 \\ \hline
        RTP & 0.0000 & 1.0000 & 0.0000 \\ \hline
        DHCP & 0.3492 & 0.1224 & 0.1813 \\ \hline
    \end{tabular}
    \label{tab:initclusterresults}
\end{table}

After the initial clustering, we perform a type distinguisher clustering in order
to refine the clustering. The results from this process are shown in
Table~\ref{tab:tdclusterresults}. Since this method is dependent on some
type distinguisher, it does not perform well on the RTP dataset. On the
other datasets, the results are close to ideal.

\begin{table}[h]
    \centering
    \captionsetup{width=0.8\textwidth}
    \caption{Clustering performance after type distinguisher clustering.}
    \begin{tabular}{| l | r | r | r | r | r |}
        \hline
        \textbf{Protocol}&\textbf{Completeness}&\textbf{Homogenity}&\textbf{V-measure} \\ \hline
        DNS & 0.9534 & 0.9999 & 0.9761 \\ \hline
        SMB & 0.9914 & 0.9880 & 0.9897 \\ \hline
        TFTP & 1.0000 & 1.0000 & 1.0000 \\ \hline
        RTP & 0.0000 & 1.0000 & 0.0000 \\ \hline
        DHCP & 1.0000 & 0.4428 & 0.6138 \\ \hline
    \end{tabular}
    \label{tab:tdclusterresults}
\end{table}

Table~\ref{tab:typedistinguishers} shows the type distinguishers chosen
by our clustering and the true type distinguishers for each protocol. Note that
we classify both type flags and request/response-flags as type distinguishers,
and that the true type distinguishers have byte resolution. The consequence of
this is that a byte which contains some bit subset that defines a type are
labeled as a true type distinguisher.

\begin{table}[h]
    \centering
    \captionsetup{width=0.8\textwidth}
    \caption{Chosen type distinguishers compared to true type distinguishers.}
    \begin{tabular}{| l | l | l |}
        \hline
        \textbf{Protocol}&\textbf{Chosen bytes}&\textbf{Actual bytes}\\ \hline
        DNS  & 2, 3         & 2, 3  \\ \hline
        SMB  & 3, 8         & 3, 8  \\ \hline
        TFTP & 1            & 1     \\ \hline
        RTP  & 8, 9, 10, 11 & 1     \\ \hline
        DHCP & 0            & 0, 242\\ \hline
    \end{tabular}
    \label{tab:typedistinguishers}
\end{table}

These results correlate with the results from Table~\ref{tab:tdclusterresults}.
If the type distinguisher bytes are chosen correctly, the clustering is close
to ideal. Failing to find a correct type distinguisher has a heavy impact on
the results. This is evident in the results from the RTP dataset. The chosen
type distinguishers are actually defined as a stream identifier in the RTP
specification. Our dataset contains several streams with similar data within
each stream, but only one actual message type.

For DHCP our type distinguisher clustering has found one of the two true
type distinguishers. The one it has found is the type byte in the
bootstrap protocol BOOTP that wraps the DHCP protocol. The second true type
byte is located in an extension header to the BOOTP protocol. Our method fails
to find this byte which causes the drop in performance for DHCP.

\section{Field inference performance}
\label{sec:fieldperf}
% Compare each byte to annotation. Measure is the percentage of correctly
% inferred bytes. Perform for both cluster and header (two different 
% subsections?)
%
% Field boundaries: Check false positives/negatives
When measuring the performance of our field inference, we do so by counting the
number of bits that match against a definition for the current protocol. To
produce a more easily comparable measure we also introduce the term
\emph{accuracy} to refer to the percentage of correct bits out of all bits that
are being looked at. The definitions for the different protocols were
constructed from openly available documentation in \citetalias{droms97},
\citetalias{sollins92}, \citetalias{mockapetris87}, \citetalias{schulzrinne03}
and \citetalias{microsoft13}. We only do these in-depth measurements for the
header of each protocol but our analysis yields similar results within clusters
as well.

\begin{table}[h]
    \centering
    \captionsetup{width=0.8\textwidth}
    \caption{Symbols used to represent the different field types ordered
    according to their precedence from highest to lowest.}
    \begin{tabular}{| l | l |}
        \hline
        \textbf{Symbol}&\textbf{Description} \\ \hline
        C   & Global constant \\ \hline
        CC  & Connection constant \\ \hline
        SC  & Stream constant \\ \hline
        L   & Global length \\ \hline
        SI  & Stream incremental \\ \hline
        N   & Global number \\ \hline
        U   & Global uniform \\ \hline
        F   & Global flag \\ \hline
        D   & Data \\ \hline
        UNK & Unknown \\ \hline
    \end{tabular}
    \label{tab:symbols}
\end{table}

For the definitions, we quantified a number of combinations of field types and
scopes into symbols. These symbols are given in order of precedence in
Table~\ref{tab:symbols}. It is worth noting that the two last field types (data
and unknown) are intrinsically non-inferable. The data field type is used in
protocol definitions when none of the other field types are applicable.
Similarly, the unknown field type is used in our field inference when the
analysis cannot reach a conclusion about a type.

We then annotated each bit of the header with these symbols. The resulting
annotations are found in Figure~\ref{fig:dnsdef} to Figure~\ref{fig:dhcpdef}.
These annotated definitions of the protocol are then compared, bit by bit,
against the result from our field inference algorithm. These resulting inferred
protocol definitions are found in Figure~\ref{fig:dnsres} to
Figure~\ref{fig:dhcpres}.

\begin{table}[h]
    \centering
    \captionsetup{width=0.8\textwidth}
    \caption{The performance of our field inference on the different datasets.}
    \begin{tabular}{| l | r | r |}
        \hline
        \textbf{Protocol}&\textbf{\# Correct Bits}&\textbf{Accuracy (\%)} \\ \hline
        DNS & 37 & 38.5 \\ \hline
        SMB & 112 & 38.9 \\ \hline
        TFTP & 0 & 0.0 \\ \hline
        RTP & 40 & 41.7 \\ \hline
        DHCP & 64 & 3.4 \\ \hline
    \end{tabular}
    \label{tab:accuracy}
\end{table}

The resulting number of correctly inferred bits and the corresponding accuracy
for the datasets are shown in Table~\ref{tab:accuracy}. At a first glance these
results seem quite mediocre. This is due to several different factors and we
will go through them each on a per dataset basis below.

\subsection{DNS performance}

% DNS header
\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \begin{bytefield}{16}
            \bitheader{0, 16}\\
            \wordbox{1}{U}\\
            \bitbox{1}{F}
            \bitbox{4}{F}
            \bitbox{1}{F}
            \bitbox{1}{F}
            \bitbox{1}{F}
            \bitbox{1}{F}
            \bitbox{3}{C}
            \bitbox{4}{F}\\
            \wordbox{1}{N}\\
            \wordbox{1}{N}\\
            \wordbox{1}{N}\\
            \wordbox{1}{N}
        \end{bytefield}
        \caption{Definition of the DNS header.}
        \label{fig:dnsdef}
    \end{subfigure}
    \quad
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \begin{bytefield}{16}
            \bitheader{0, 16}\\
            \wordbox{1}{U}\\
            \bitbox{8}{SC}
            \bitbox{8}{F}\\
            \wordbox{1}{C}\\
            \bitbox{8}{C}
            \bitbox{8}{N}\\
            \bitbox{8}{C}
            \bitbox{8}{N}\\
            \bitbox{8}{C}
            \bitbox{8}{UNK}
        \end{bytefield}
        \caption{The result from our inference of the DNS header.}
        \label{fig:dnsres}
    \end{subfigure}
    \caption{Comparison of the actual header and the inferred header of DNS.}
    \label{fig:dnsperf}
\end{figure}

The main reason for the suboptimal performance in the DNS case is the lack of
diversity in the packet dump. As we can see in Table~\ref{tab:symbols}, the
three types of constant fields has higher precedence than any other type of
field. This is the reason for the fields incorrectly classified as C and SC in
Figure~\ref{fig:dnsres} as they are constant within their scopes in our dump.
Another factor is our precision; we see that the fourth byte has bits of
different types, both flag and constant, but our method only looks at them on
a byte level. This causes the whole byte to be classified as a flag as the
constant bits do not interfere with our classification for flag fields. We can
also see that the last byte is not classified as a number because its
distribution does not fit well to a normal distribution, again due to low
diversity.

\subsection{SMB performance}

% Comment that a stream constant (such as security features) is classified
% as a global constant if it is the same constant in all streams in the
% dump.

% SMB header
\newcommand{\colorbitbox}[3]{%
    \rlap{\bitbox{#2}{\color{#1}\rule{\width}{\height}}}%
    \bitbox{#2}{#3}}
\definecolor{lightgray}{gray}{0.9}
\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \begin{bytefield}[bitwidth=0.5em]{32}
            \bitheader{0, 31}\\
            \colorbitbox{lightgray}{8}{F} & \colorbitbox{lightgray}{24}{L}\\
            \wordbox{1}{C}\\    % 0xffSMB
            \bitbox{8}{F}       % Command
            \bitbox{24}{F}\\    % Status
            \bitbox{8}{...}     % (continuation)
            \bitbox{8}{F}       % Flags
            \bitbox{16}{F}\\    % Flags2
            \bitbox{16}{D}     % PID High
            \bitbox{16}{D}\\   % SecurityFeatures
            \wordbox{1}{...}\\  % (continuation)
            \bitbox{16}{...}    % (continuation)
            \bitbox{16}{C}\\    % Reserved
            \bitbox{16}{D}      % Tree ID
            \bitbox{16}{D}\\   % PID Low
            \bitbox{16}{D}      % User ID
            \bitbox{16}{D}     % Multiplex ID
        \end{bytefield}
        \caption{Definition of the SMB header.}
        \label{fig:smbdef}
    \end{subfigure}
    \quad
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \begin{bytefield}[bitwidth=0.5em]{32}
            \bitheader{0, 31}\\
            \colorbitbox{lightgray}{8}{C}
            \colorbitbox{lightgray}{8}{F}
            \colorbitbox{lightgray}{16}{L}\\
            \wordbox{1}{C}\\    % 0xffSMB
            \bitbox{8}{UNK}     % Command
            \bitbox{8}{UNK}     % Command
            \bitbox{16}{F}\\    % Status
            \wordbox{1}{F}\\    
            \bitbox{8}{N}     % PID High
            \bitbox{8}{C}   % SecurityFeatures
            \bitbox{16}{C}\\   % SecurityFeatures
            \wordbox{1}{C}\\  % (continuation)
            \wordbox{1}{C}\\  % (continuation)
            \wordbox{1}{N}\\  % (continuation)
            \bitbox{8}{UNK}      % Tree ID
            \bitbox{8}{F}   % PID Low
            \bitbox{8}{U}      % User ID
            \bitbox{8}{N}     % Multiplex ID
        \end{bytefield}
        \caption{The result from our inference of the SMB header.}
        \label{fig:smbres}
    \end{subfigure}
    \caption{Comparison of the actual header and the inferred header of SMB.
        Fields which belong to the NetBIOS-header are marked with a gray
        background. Dots denote continuation of the previous field.}
    \label{fig:smbperf}
\end{figure}

Despite similar accuracy, the performance of SMB is actually quite a lot
better. The main culprit behind the low performance here lies instead in the
protocol definition. The SMB header has a lot of data fields which are, as
previously mentioned, non-inferable. This means that actually only 144 of its
288 bits can be found. If we had chosen to define accuracy on these bits
instead we would get an accuracy of 77.8\%. There is also a slight problem with
diversity again, causing some fields to be incorrectly classified as global
constant.

\subsection{TFTP performance}

% TFTP header
\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \begin{bytefield}{16}
            \bitheader{0, 15}\\
            \wordbox{1}{F}    % OPCode
        \end{bytefield}
        \caption{Definition of the TFTP header.}
        \label{fig:tftpdef}
    \end{subfigure}
    \quad
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \begin{bytefield}{16}
            \bitheader{0, 15}\\
            \bitbox{8}{C}
            \bitbox{8}{SC}
        \end{bytefield}
        \caption{The result from our inference of the TFTP header.}
        \label{fig:tftpres}
    \end{subfigure}
    \caption{Comparison of the actual header and the inferred header of TFTP.}
    \label{fig:tftpperf}
\end{figure}

We get the absolute worst performance with TFTP with no correct bits at all.
Yet again the problem lies in the diversity of the dump. There are only two
bytes in the TFTP header and both are classified as a type of constant. The
performance is on the other hand better within the clusters as other fields are
found.

\subsection{RTP performance}

% Comment for RTP: One stream messed up the incremental in sequence number.
% Comment on how consensus of all streams may not be optimal for finding 
% correct semantics.

% RTP header
\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \begin{bytefield}[bitwidth=0.5em]{32}
            \bitheader{0, 31}\\
            \bitbox{2}{\tiny{F}}       % Ver
            \bitbox{1}{\tiny{F}}       % P
            \bitbox{1}{\tiny{F}}       % X
            \bitbox{4}{L}       % CC
            \bitbox{1}{\tiny{F}}       % M
            \bitbox{7}{F}       % PT
            \bitbox{16}{SI}\\   % Seq. nbr
            \wordbox{1}{SI}\\   % Timestamp
            \wordbox{1}{SC}   % SSRC
        \end{bytefield}
        \caption{Definition of the RTP header.}
        \label{fig:rtpdef}
    \end{subfigure}
    \quad
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \begin{bytefield}[bitwidth=0.5em]{32}
            \bitheader{0, 31}\\
            \bitbox{8}{C}
            \bitbox{8}{F}
            \bitbox{16}{N}\\
            \bitbox{8}{C}
            \bitbox{8}{UNK}
            \bitbox{8}{U}
            \bitbox{8}{UNK}\\
            \wordbox{1}{SC}
        \end{bytefield}
        \caption{The result from our inference of the RTP header.}
        \label{fig:rtpres}
    \end{subfigure}
    \caption{Comparison of the actual header and the inferred header of RTP.}
    \label{fig:rtpperf}
\end{figure}

The best performance for our field inference was found in the RTP dump. When
examined more closely it could have been even better as the stream incremental
fields was nearly identified as well. The reason why it was not is that we
enforce all streams to be in agreement in order for a field to be classified as
something with a stream scope. In this case, one of the streams had a lot of
retransmitted packets that caused the field to not increase so that it could
not be classified as incremental. The condition that all streams need to be in
agreement to classify a field might be too strict. A more reasonable approach
could be to only require a majority of agreeing streams. Another smaller source
of error is that there is yet again some fields that are constant within the
dump causing incorrect classifications.

\subsection{DHCP performance}

% DHCP header
\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \begin{bytefield}[bitwidth=0.5em]{32}
            \bitheader{0, 31}\\
            \bitbox{8}{F}       % OPCode
            \bitbox{8}{F}       % HWType
            \bitbox{8}{F}       % HW addr. len
            \bitbox{8}{N}\\     % Hop-count
            \wordbox{1}{U}\\    % Transaction ID
            \bitbox{16}{D}      % Num seconds
            \bitbox{16}{F}\\    % Flags
            \wordbox{1}{D}\\    % Client IP
            \wordbox{1}{D}\\    % Your IP
            \wordbox{1}{D}\\    % Server IP
            \wordbox{1}{D}\\    % Gateway IP
            \wordbox{1}{D (16 bytes)}\\  % Client HW Addr
            \wordbox{1}{D (64 bytes)}\\  % Client HW Addr
            \wordbox{1}{D (128 bytes)}\\  % Client HW Addr
            \wordbox[]{9}{}
        \end{bytefield}
        \caption{Definition of the DHCP header.}
        \label{fig:dhcpdef}
    \end{subfigure}
    \quad
    \begin{subfigure}[t]{0.48\textwidth}
        \begin{bytefield}[bitwidth=0.5em]{32}
            \bitheader{0, 31}\\
            \bitbox{8}{SC}       % OPCode
            \bitbox{8}{F}      % HWType
            \bitbox{8}{F}       % HW addr. len
            \bitbox{8}{N}\\     % Hop-count
            \wordbox{1}{U}\\   % Transaction ID
            \bitbox{16}{UNK}       % HW addr. len
            \bitbox{8}{F}       % HW addr. len
            \bitbox{8}{C}\\       % HW addr. len
            \wordbox{1}{UNK (8 bytes)}\\
            \bitbox{8}{F}       % HW addr. len
            \bitbox{24}{UNK}\\       % HW addr. len
            \bitbox{8}{F}       % HW addr. len
            \bitbox{16}{UNK}       % HW addr. len
            \bitbox{8}{F}\\       % HW addr. len
            \wordbox{1}{UNK}\\
            \bitbox{16}{UNK}       % HW addr. len
            \bitbox{16}{C}\\       % HW addr. len
            \wordbox{1}{C (8 bytes)}\\
            \wordbox{1}{SC (8 bytes)}\\
            \bitbox{16}{SC}
            \bitbox{8}{SC}
            \bitbox{8}{CC}\\
            \wordbox{1}{CC (12 bytes)}\\
            \wordbox{1}{C (40 bytes)}\\
            \wordbox{1}{UNK (12 bytes)}\\
            \bitbox{24}{UNK}
            \bitbox{8}{F}\\
            \wordbox{1}{F (12 bytes)}\\
            \bitbox{16}{F}
            \bitbox{8}{F}
            \bitbox{8}{C}\\
            \wordbox{1}{C (92 bytes)}\\
            \bitbox{16}{C}
            \bitbox{8}{C}
            \bitbox{8}{F}
        \end{bytefield}
        \caption{The result from our inference of the DHCP header.}
        \label{fig:dhcpres}
    \end{subfigure}
    \caption{Comparison of the actual header and the inferred header of DHCP.
        Note that the header is too long to show here in its actual size; the
        length in bytes for some of the fields are shown in text instead.}
    \label{fig:dhcpperf}
\end{figure}

The main problem with the classification in the case of DHCP is the extremely
large amount of data in the header. Only 80 of the 1888 bits in the header is
actually classifiable. If we consider this the accuracy is actually 80.0\%.

The fields that are specified as data does not have a clear enough definition
in its documentation to fall into one of our categories. In reality they might
have characteristics that does match one of our categories so some of the
fields that are classified as data in the definition and something else in
our inferred results might actually be correct.

\chapter{Discussion}

\section{Conclusions}
% Explain the excellent results from OPTICS clustering
% Discuss the problems with type distinguisher clustering for protocols that
% have only one type
% Emphasize the importance of diverse dumps (many different types, edge cases,
% sessions etc.)
In this thesis, we have shown how density-based clustering algorithms may be
applied to binary network protocols, and how protocol field semantics may be
inferred. Our performance measurements in Section~\ref{sec:clustperf} indicate
that our method is capable of very good results given the existence of some
type distinguisher. It also shows the difficulties in trying to infer types
in protocols where no type distinguisher exists or when the dataset is
not diverse enough.

Having diverse datasets is key to constructing a good type inference. Ideally, a
dataset containing all possible message types, a large number of sessions,
and messages from all edge-cases of the protocol would be needed for a
complete inference. This would require large datasets containing many more
messages than the datasets we used during our evaluation. Using the OPTICS
algorithm for such large datasets would be infeasible since its time
complexity is at best $O(n \cdot \log n)$ as proven by \citet{ankerst99}.
A better approach would be to perform OPTICS on a subset of a large dataset
in order to find the format distinguishers, and subsequently perform
format distinguisher clustering on the complete dataset.

% State machines
From the inferred types we were able to construct state diagrams that reveal
the order in which messages of different types were sent. This was suggested
as future work in Discoverer by \citet{cui07}. For protocols with a large
number of types we infer an immense amount of state transitions. This renders
a very complex state diagram. Instead we consider the state diagram inference
useful when examining protocol initiation procedures, or protocols such as
DNS where the number of types are small.

% Explain why the field inferences are actually pretty good, and why it is hard
% to define a metric which is suitable.
When the types were inferred we could proceed to infer fields and field
semantics. In Section~\ref{sec:field_inf} we introduced a set of
field primitives and corresponding heuristics for classifying them. The
performance of these heuristics was measured in Section~\ref{sec:fieldperf}.
The metric we introduced for measuring field inference performance was very
strict in the sense that ideal results (100\% correct field classifications)
were in most cases only achievable if the actual protocol specification was
given as input. For instance, fields labeled as data in the true specification
could never be classified correctly, thus making a perfect classification
impossible. Taking this into account, our field inference performance is
fairly good. We have not found any related works that are able to infer field
semantics at the same level of detail as our method.

The time consumption for our field inference is rather high, mainly because
of the classification of number fields which performs a large number of normal
distribution curve fittings.

After inferring fields in common for all message types, we proceeded to
infer fields within each type. During this process we aligned the data using
our modified version of Needleman-Wunsch defined in
Section~\ref{sec:nonmutualalign}. After the data had been aligned, we attempted
to perform our field inference on the aligned data with mixed results. One
of the major problems with validating the results for the field inference
within clusters is the need for annotated data. The large number of different
message types makes it difficult to create definitions for all of them. Another
problem is the impact of incorrect alignments. A few misaligned messages can
cause confusion for our classification heuristics.

Finding an alternative to using sequence alignment when examining variable
length fields or a variable number of fields is an area we leave to future work.

\section{Limitations}
During the development of our approach we have both found a number of
limitations and introduced a number of restrictions. We focus on binary
protocols where the fields are aligned. The main limitations of our approach
is listed in this section.

\subsection{Textual protocols}
Our method attempts to find structure and semantics in binary protocols.
By nature, binary protocols are hard to read and it may be impossible to
uncover the exact meaning of every field. Textual protocols, on the other
hand, are often self-explanatory. Often they are designed to be read by
humans. Textual protocols work well in applications that have no constraint
on message size or parsing performance. Notations commonly used in textual
protocols such as JSON and XML are more complex to parse than binary protocols,
and introduces features where our approach is not feasible.

In Discoverer by \citeauthor{cui07} an attempt to infer the structure of
the HTTP protocol was made. The HTTP header contains a set of unordered
key-value pairs delimited by line breaks. Due to the pairs being unordered,
the method yielded an immense number of possible types.

For the type inference, our method could be able to find some type
distinguisher, perhaps the first three ASCII characters of each HTTP method.
Our field inference would however not work, since we are assuming aligned
data in a specific ordering.

\subsection{Variable number of fields}
A variable number of fields may be useful in a protocol specification. BOOTP,
which is the parent protocol to DHCP, has a variable number of header 
extensions. DNS has a variable number of queries or answers. How the existence
of a variable number of fields is specified is protocol specific, thus hard to
determine analytically.

There are byte patterns which could be analyzed in order to infer optional
fields, such as length fields or some byte value that decides the number of
extended fields. This type of analysis has not been taken into account as we
deem it out of the scope for this thesis.

\subsection{Non-aligned data}
In our approach, we assume that each field is aligned to one, two, or four
bytes. This is somewhat of a standard as explained in
Section~\ref{sec:simplifications}. Although there are obvious benefits to
aligning the fields in a protocol, not every protocol follows the field
alignment concept. SMB is one of those protocols.

If some field is six bytes long, our method will at best detect it as a
two-byte field and a four-byte field. If we were to look for fields with
all possible alignments, we would have to use a sliding window approach where
we would have to test a six-byte window for every possible offset in order
to find a six-byte field. This would increase the complexity of the field
inference immensely, since we would have to try $length - n$ possible fields
for every length $n$. With our approach, we try at most
$\lfloor \frac{length}{n} \rfloor$ possible fields for every length $n$.

\subsection{Endianness}
Many network protocols use big-endian order for their data, as suggested
in the guidelines in RFC1700 by \citet{reynolds94}. However, some protocols
still use little-endian order partially or completely in their specifications.
Our method assumes big-endian order.

\subsection{Bit precision}
It is not uncommon for fields to have a higher resolution than one byte. In
the RTP header visualized in Figure~\ref{fig:rtpdef} there is a four-byte
length field between flag-bits in the first byte. In order to find these
fields we would have to introduce bit precision in our analysis. Bit
precision analysis would introduce the same complexity issues as non-aligned
data analysis.

% Skriv mer hr

\section{Future work}
There are a number of areas we have not been able to explore in this thesis
due to limited time or resources. After evaluating our method we propose the
following improvements.

\subsection{Iterative identification of length fields}
A possible alternative to using sequence alignment could be to find the exact
length of all variable parts for each message. This could be done by performing
identification of length field in an iterative manner. Each subsequent iteration
would then use the parts of the message that has not yet been included into the
range of a previously identified length field.

\subsection{Correspondence analysis}
In Section~\ref{sec:pca} we describe how we use PCA in order to reduce the
dimensionality of our features while preserving the components that contribute
with the most variance. Another method which falls under the same category is
correspondence analysis. Correspondence analysis is very similar to PCA, but
it is applied to nominal data instead of continuous data.

Using correspondence analysis we could possibly use nominal data for our
features in Section~\ref{sec:init_clust} instead of probabilities. Due to the
lack of correspondence analysis libraries we have been unable to evaluate its
performance. Judging from its definition, it should be more suitable than PCA
in our field of application.

\subsection{Machine learning for field inference}
Our methods use statistical heuristics when inferring fields. Each heuristic
is designed for a certain type of field class based on the characteristics we
have seen from its byte value distribution. A possible alternative to these
heuristics would be to use classification concepts from machine learning such
as logistic regression or support vector machines.

In our research we considered using these methods when classifying fields.
The main complication with this approach was the difficulty in building the
training set. Each byte in a protocol yields a single sample. In order to
build a good training set we would have to annotate and process a large
amount of binary protocols. These techniques could possibly lead to a
more robust field classification. Building the training set is however out
of the scope for this thesis.

\subsection{Timestamp identification}
From our analysis, we have seen that timestamps have a potentially
distinguishable byte distribution. This distribution could perhaps be analyzed
in order to classify a field as a timestamp. The distribution reveals a number
of spikes we assume to be dependent on the time resolution. This is most
visible in the least significant byte of a timestamp field. Using frequency
analysis, it should be possible to determine the time resolution. This could
be applicable for media streaming protocols.

For general timestamps, we could compare the difference in packet delivery
timestamps given by the dataset with the difference of the values in the field.
If the relationship between the two differences consistent enough, we could
classify the field as a timestamp.

\bibliographystyle{plainnat}
\bibliography{thesis}

\end{document}

